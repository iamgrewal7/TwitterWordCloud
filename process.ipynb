{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPT 459 Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Name -> Harman Singh*\n",
    "### *ID -> 301326898*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-9a1aa3820396>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-9a1aa3820396>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    import pandas as pdsorted(dic, key=operator.itemgetter(1))\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Standard Library\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "\n",
    "# Third Party\n",
    "import pandas as pdsorted(dic, key=operator.itemgetter(1))\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from langdetect import detect\n",
    "from texttable import Texttable\n",
    "from matplotlib_venn_wordcloud import venn2_wordcloud\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "    \"\"\"This helper method removes links, RT, hashtags, punctuations from text\n",
    "    \"\"\"\n",
    "    # Remove RT @user: user\n",
    "    text = re.sub(r'(^RT )?@[A-Za-z0-9]+(:)?', \"\", text)\n",
    "    \n",
    "    # Remove html tags like &amp\n",
    "    text = re.sub(r'&[a-z]+', '', text)\n",
    "    \n",
    "    # Remove links\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+',\"\", text)\n",
    "    \n",
    "    # Lowercase Everything\n",
    "    text = text.lower()\n",
    "    \n",
    "    # https://stackoverflow.com/a/33417311\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # Remove unicode punctuation\n",
    "    text = remove_punctuation(text)\n",
    "    \n",
    "    # Get rid of all spaces in the beginning or end\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def number_of_unique_tokens(tokens):\n",
    "    counter = Counter(tokens)\n",
    "    return len(counter)\n",
    "\n",
    "\n",
    "def print_most_frequent(_dict, n):\n",
    "    table = Texttable()\n",
    "    table.header(['Token', 'Normalized Frequency'])\n",
    "    for word, freq in _dict.most_common(n):\n",
    "        table.add_row([word, freq/1000])\n",
    "    print(table.draw())\n",
    "\n",
    "\n",
    "# Taken from TA's video\n",
    "def detect_language(token):\n",
    "    try:\n",
    "        return detect(token)\n",
    "    except:\n",
    "        return 'Other'\n",
    "    \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stop_words(text):\n",
    "    \"\"\" This functions filters out all english stop words from text\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(text)\n",
    "    filtered = [\n",
    "        token for token in tokenized if (\n",
    "            len(token) > 2 and\n",
    "            token not in stop_words and\n",
    "            token.isalnum() and\n",
    "            token != 'nan'\n",
    "        )\n",
    "    ]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/a/11066687\n",
    "# Helper for removing unicode punctuation\n",
    "tbl = dict.fromkeys(\n",
    "    i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P')\n",
    ")\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('D1.csv')\n",
    "df2 = pd.read_csv('D2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data to remove all punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process D1\n",
    "df1['filtered'] = df1['text'].apply(filter_text)\n",
    "df1['lang'] = df1['filtered'].apply(detect_language)\n",
    "df1 = df1[df1['lang'] == 'en']\n",
    "df1['without_stop_words'] = df1['filtered'].apply(remove_stop_words)\n",
    "\n",
    "# Process D2\n",
    "df2['filtered'] = df2['text'].apply(filter_text)\n",
    "df2['lang'] = df2['filtered'].apply(detect_language)\n",
    "df2 = df2[df2['lang'] == 'en']\n",
    "df2['without_stop_words'] = df2['filtered'].apply(remove_stop_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 3.1 Number of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'number_of_unique_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b9945e5109c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munique_tokens_D1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumber_of_unique_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'without_stop_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0munique_tokens_D2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumber_of_unique_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'without_stop_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Number of Unique Tokens in D1: {unique_tokens_D1}, Number of Unique Tokens in D2: {unique_tokens_D2}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'number_of_unique_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "unique_tokens_D1 = number_of_unique_tokens(df1['without_stop_words'].explode().unique())\n",
    "unique_tokens_D2 = number_of_unique_tokens(df2['without_stop_words'].explode().unique())\n",
    "\n",
    "print(f'Number of Unique Tokens in D1: {unique_tokens_D1}, Number of Unique Tokens in D2: {unique_tokens_D2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 3.2 Top-100 most frequent tokenss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_D1 = nltk.FreqDist(list(df1['without_stop_words'].explode()))\n",
    "print('100 Most Frequent Tokens in D1:')\n",
    "print_most_frequent(freq_D1, 100)\n",
    "\n",
    "freq_D2 = nltk.FreqDist(list(df2['without_stop_words'].explode()))\n",
    "print('100 Most Frequent Tokens in D2:')\n",
    "print_most_frequent(freq_D2, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 3.3 Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this part I use python word cloud library to create word cloud from frequency dictionary generated from nltk\n",
    "# Word Cloud that gets generated has random colors but the relative size of word is bigger for more frequent word\n",
    "\n",
    "wc = WordCloud()\n",
    "fig = plt.figure(figsize=(30, 30))\n",
    "\n",
    "# Plot D1\n",
    "wc.generate_from_frequencies(freq_D1)\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Words from random tweets (D1)\", fontSize=24)\n",
    "\n",
    "# Plot D2\n",
    "wc.generate_from_frequencies(freq_D2)\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Words from covid tweets (D2)\", fontSize=24)\n",
    "\n",
    "plt.show(block=True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 3.4 Comparing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For comparing the words from D1 and D2, I chose to draw Venn Diagram.\n",
    "# Sizes are random, since I am not passing frequency\n",
    "# (there were some bugs with library when I passed frequency)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(20, 20))\n",
    "axes.set_title('Comparing Top 50 Words from D1 and D2', fontsize=20)\n",
    "\n",
    "# This libray creates venn diagram and fill them with word cloud\n",
    "venn = venn2_wordcloud(\n",
    "    [\n",
    "        set(key for key, _ in freq_D1.most_common(50)),\n",
    "        set(key for key, _ in freq_D2.most_common(50))\n",
    "    ],\n",
    "    ax=axes,\n",
    "    set_labels=['D1', 'D2'],\n",
    ")\n",
    "\n",
    "# Set Colors\n",
    "venn.get_patch_by_id('10').set_color('blue')\n",
    "venn.get_patch_by_id('10').set_alpha(0.4)\n",
    "venn.get_patch_by_id('01').set_color('red')\n",
    "venn.get_patch_by_id('01').set_alpha(0.4)\n",
    "venn.get_patch_by_id('11').set_color('purple')\n",
    "venn.get_patch_by_id('11').set_alpha(0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 3.5 Link to Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Link to Tweet](https://twitter.com/iamgrewal7/status/1267226701236334592?s=20 \"Twitter\")\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
